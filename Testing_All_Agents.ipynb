{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bef06377",
   "metadata": {},
   "source": [
    "# 🧪 Agentic AI Scheduler - Comprehensive Testing\n",
    "\n",
    "## AMD Hackathon 2025 - Agent Testing Suite\n",
    "\n",
    "This notebook provides comprehensive testing for all agentic AI components:\n",
    "- **Agent Information Extraction** testing\n",
    "- **OR-Tools Optimization** validation\n",
    "- **Performance benchmarking** and latency analysis\n",
    "- **Edge case handling** verification\n",
    "- **Integration testing** across components\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "680b4881",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Failed to import AgenticScheduler: No module named 'agentic_scheduler'\n",
      "Please ensure agentic_scheduler.py is in the current directory\n",
      "🧪 Agentic AI Testing Suite - AMD Hackathon 2025\n",
      "🎯 Comprehensive validation of all agent components\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Test Environment Setup\n",
    "import json\n",
    "import time\n",
    "import sys\n",
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "from typing import Dict, List, Any\n",
    "import unittest\n",
    "from unittest.mock import patch, MagicMock\n",
    "\n",
    "# Import our agentic scheduler\n",
    "try:\n",
    "    from agentic_scheduler import AgenticScheduler\n",
    "    print(\"✅ Agentic Scheduler imported successfully\")\n",
    "except ImportError as e:\n",
    "    print(f\"❌ Failed to import AgenticScheduler: {e}\")\n",
    "    print(\"Please ensure agentic_scheduler.py is in the current directory\")\n",
    "\n",
    "print(\"🧪 Agentic AI Testing Suite - AMD Hackathon 2025\")\n",
    "print(\"🎯 Comprehensive validation of all agent components\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1985685",
   "metadata": {},
   "source": [
    "## 🤖 Agent Extraction Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fca0d3fd",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'AgenticScheduler' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 140\u001b[39m\n\u001b[32m    137\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[32m    139\u001b[39m \u001b[38;5;66;03m# Run extraction tests\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m140\u001b[39m extraction_tester = \u001b[43mTestAgentExtraction\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    141\u001b[39m extraction_results = extraction_tester.run_extraction_tests()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 5\u001b[39m, in \u001b[36mTestAgentExtraction.__init__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m     \u001b[38;5;28mself\u001b[39m.scheduler = \u001b[43mAgenticScheduler\u001b[49m(agentic_mode=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m      6\u001b[39m     \u001b[38;5;28mself\u001b[39m.test_cases = [\n\u001b[32m      7\u001b[39m         {\n\u001b[32m      8\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mname\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mExplicit Duration and Urgency\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     46\u001b[39m         }\n\u001b[32m     47\u001b[39m     ]\n",
      "\u001b[31mNameError\u001b[39m: name 'AgenticScheduler' is not defined"
     ]
    }
   ],
   "source": [
    "class TestAgentExtraction:\n",
    "    \"\"\"Test suite for agent-based information extraction.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.scheduler = AgenticScheduler(agentic_mode=True)\n",
    "        self.test_cases = [\n",
    "            {\n",
    "                \"name\": \"Explicit Duration and Urgency\",\n",
    "                \"content\": \"URGENT: Need to schedule a 2-hour crisis meeting ASAP tomorrow morning. All hands on deck!\",\n",
    "                \"expected\": {\n",
    "                    \"meeting_duration_minutes\": 120,\n",
    "                    \"time_preference\": \"morning\",\n",
    "                    \"urgency\": \"high\",\n",
    "                    \"meeting_type\": \"team_meeting\"\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"Implicit Information\",\n",
    "                \"content\": \"Let's catch up sometime this week. Nothing urgent, just a quick sync.\",\n",
    "                \"expected\": {\n",
    "                    \"meeting_duration_minutes\": 30,  # Should infer short duration\n",
    "                    \"time_preference\": \"anytime\",\n",
    "                    \"urgency\": \"low\",\n",
    "                    \"meeting_type\": \"team_meeting\"\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"Client Meeting Context\",\n",
    "                \"content\": \"Please schedule a client presentation for this afternoon. We need 90 minutes to cover everything.\",\n",
    "                \"expected\": {\n",
    "                    \"meeting_duration_minutes\": 90,\n",
    "                    \"time_preference\": \"afternoon\",\n",
    "                    \"urgency\": \"medium\",\n",
    "                    \"meeting_type\": \"presentation\"\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"Interview Scheduling\",\n",
    "                \"content\": \"We need to schedule technical interviews for the senior engineer position. Each should be about 1 hour.\",\n",
    "                \"expected\": {\n",
    "                    \"meeting_duration_minutes\": 60,\n",
    "                    \"time_preference\": \"anytime\",\n",
    "                    \"urgency\": \"medium\",\n",
    "                    \"meeting_type\": \"interview\"\n",
    "                }\n",
    "            }\n",
    "        ]\n",
    "    \n",
    "    def run_extraction_tests(self):\n",
    "        \"\"\"Run all extraction tests.\"\"\"\n",
    "        print(\"🧠 Testing Agent Information Extraction...\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        results = []\n",
    "        total_time = 0\n",
    "        \n",
    "        for i, test_case in enumerate(self.test_cases, 1):\n",
    "            print(f\"\\n🧪 Test {i}: {test_case['name']}\")\n",
    "            print(f\"📧 Content: {test_case['content'][:60]}...\")\n",
    "            \n",
    "            start_time = time.time()\n",
    "            \n",
    "            try:\n",
    "                if self.scheduler.agentic_mode and self.scheduler.agentic_ready:\n",
    "                    extracted = self.scheduler._extract_meeting_info_with_agent(test_case['content'])\n",
    "                    method = \"Agent-based\"\n",
    "                else:\n",
    "                    extracted = self.scheduler._extract_meeting_info_traditional(test_case['content'])\n",
    "                    method = \"Traditional\"\n",
    "                \n",
    "                extraction_time = time.time() - start_time\n",
    "                total_time += extraction_time\n",
    "                \n",
    "                print(f\"✅ Extraction completed ({method}) in {extraction_time:.3f}s\")\n",
    "                print(f\"📊 Extracted: {json.dumps(extracted, indent=2)}\")\n",
    "                \n",
    "                # Validate against expected results\n",
    "                expected = test_case['expected']\n",
    "                accuracy_score = 0\n",
    "                total_checks = len(expected)\n",
    "                \n",
    "                for key, expected_value in expected.items():\n",
    "                    if key in extracted:\n",
    "                        extracted_value = extracted[key]\n",
    "                        if isinstance(expected_value, int):\n",
    "                            # Allow some tolerance for duration estimates\n",
    "                            if abs(extracted_value - expected_value) <= 30:\n",
    "                                accuracy_score += 1\n",
    "                                print(f\"   ✅ {key}: {extracted_value} (expected ~{expected_value})\")\n",
    "                            else:\n",
    "                                print(f\"   ❌ {key}: {extracted_value} (expected ~{expected_value})\")\n",
    "                        else:\n",
    "                            if extracted_value == expected_value:\n",
    "                                accuracy_score += 1\n",
    "                                print(f\"   ✅ {key}: {extracted_value}\")\n",
    "                            else:\n",
    "                                print(f\"   ❌ {key}: {extracted_value} (expected {expected_value})\")\n",
    "                    else:\n",
    "                        print(f\"   ❌ {key}: Missing\")\n",
    "                \n",
    "                accuracy = (accuracy_score / total_checks) * 100\n",
    "                print(f\"📈 Accuracy: {accuracy:.1f}% ({accuracy_score}/{total_checks})\")\n",
    "                \n",
    "                results.append({\n",
    "                    'name': test_case['name'],\n",
    "                    'extraction_time': extraction_time,\n",
    "                    'accuracy': accuracy,\n",
    "                    'method': method,\n",
    "                    'success': True\n",
    "                })\n",
    "                \n",
    "            except Exception as e:\n",
    "                extraction_time = time.time() - start_time\n",
    "                print(f\"❌ Extraction failed: {e}\")\n",
    "                results.append({\n",
    "                    'name': test_case['name'],\n",
    "                    'extraction_time': extraction_time,\n",
    "                    'accuracy': 0,\n",
    "                    'method': 'Failed',\n",
    "                    'success': False,\n",
    "                    'error': str(e)\n",
    "                })\n",
    "        \n",
    "        # Summary\n",
    "        print(f\"\\n📈 EXTRACTION TEST SUMMARY\")\n",
    "        print(f\"=\" * 30)\n",
    "        successful = sum(1 for r in results if r['success'])\n",
    "        avg_time = total_time / len(results)\n",
    "        avg_accuracy = sum(r['accuracy'] for r in results if r['success']) / max(successful, 1)\n",
    "        \n",
    "        print(f\"📊 Tests run: {len(results)}\")\n",
    "        print(f\"✅ Successful: {successful}/{len(results)} ({successful/len(results)*100:.1f}%)\")\n",
    "        print(f\"⏱️ Average time: {avg_time:.3f} seconds\")\n",
    "        print(f\"🎯 Average accuracy: {avg_accuracy:.1f}%\")\n",
    "        print(f\"🚀 Ready for production: {'✅ YES' if successful == len(results) and avg_accuracy >= 80 else '❌ NO'}\")\n",
    "        \n",
    "        return results\n",
    "\n",
    "# Run extraction tests\n",
    "extraction_tester = TestAgentExtraction()\n",
    "extraction_results = extraction_tester.run_extraction_tests()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4822bd22",
   "metadata": {},
   "source": [
    "## 🔍 OR-Tools Optimization Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf43173e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestOROptimization:\n",
    "    \"\"\"Test suite for OR-Tools based optimization.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.scheduler = AgenticScheduler(agentic_mode=True)\n",
    "        self.test_scenarios = [\n",
    "            {\n",
    "                \"name\": \"Simple 2-Person Meeting\",\n",
    "                \"attendees\": [\"user1@amd.com\", \"user2@amd.com\"],\n",
    "                \"duration\": 60,\n",
    "                \"preference\": \"morning\",\n",
    "                \"expected_constraints\": [\"business_hours\", \"no_conflicts\", \"preference_match\"]\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"Large Team Meeting\",\n",
    "                \"attendees\": [f\"user{i}@amd.com\" for i in range(1, 8)],\n",
    "                \"duration\": 90,\n",
    "                \"preference\": \"afternoon\",\n",
    "                \"expected_constraints\": [\"business_hours\", \"no_conflicts\", \"duration_fit\"]\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"Quick 15-min Standup\",\n",
    "                \"attendees\": [\"lead@amd.com\", \"dev1@amd.com\", \"dev2@amd.com\"],\n",
    "                \"duration\": 15,\n",
    "                \"preference\": \"anytime\",\n",
    "                \"expected_constraints\": [\"business_hours\", \"short_duration\"]\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"Long Workshop\",\n",
    "                \"attendees\": [\"trainer@amd.com\", \"team@amd.com\"],\n",
    "                \"duration\": 240,  # 4 hours\n",
    "                \"preference\": \"morning\",\n",
    "                \"expected_constraints\": [\"business_hours\", \"long_duration\", \"preference_match\"]\n",
    "            }\n",
    "        ]\n",
    "    \n",
    "    def run_optimization_tests(self):\n",
    "        \"\"\"Run all optimization tests.\"\"\"\n",
    "        print(\"🔍 Testing OR-Tools Optimization...\")\n",
    "        print(\"=\" * 40)\n",
    "        \n",
    "        results = []\n",
    "        total_time = 0\n",
    "        \n",
    "        for i, scenario in enumerate(self.test_scenarios, 1):\n",
    "            print(f\"\\n🧪 Test {i}: {scenario['name']}\")\n",
    "            print(f\"👥 Attendees: {len(scenario['attendees'])}\")\n",
    "            print(f\"⏰ Duration: {scenario['duration']} minutes\")\n",
    "            print(f\"🕐 Preference: {scenario['preference']}\")\n",
    "            \n",
    "            start_time = time.time()\n",
    "            \n",
    "            try:\n",
    "                optimal_start, optimal_end = self.scheduler.find_optimal_slot(\n",
    "                    attendees=scenario['attendees'],\n",
    "                    duration_minutes=scenario['duration'],\n",
    "                    time_preference=scenario['preference'],\n",
    "                    start_date=datetime.now().isoformat()\n",
    "                )\n",
    "                \n",
    "                optimization_time = time.time() - start_time\n",
    "                total_time += optimization_time\n",
    "                \n",
    "                print(f\"✅ Optimization completed in {optimization_time:.3f}s\")\n",
    "                print(f\"📅 Optimal slot: {optimal_start} to {optimal_end}\")\n",
    "                \n",
    "                # Validate the solution\n",
    "                validation_score = self._validate_solution(\n",
    "                    optimal_start, optimal_end, scenario\n",
    "                )\n",
    "                \n",
    "                print(f\"📊 Validation score: {validation_score:.1f}%\")\n",
    "                \n",
    "                results.append({\n",
    "                    'name': scenario['name'],\n",
    "                    'optimization_time': optimization_time,\n",
    "                    'validation_score': validation_score,\n",
    "                    'start_time': optimal_start,\n",
    "                    'end_time': optimal_end,\n",
    "                    'success': True\n",
    "                })\n",
    "                \n",
    "            except Exception as e:\n",
    "                optimization_time = time.time() - start_time\n",
    "                print(f\"❌ Optimization failed: {e}\")\n",
    "                results.append({\n",
    "                    'name': scenario['name'],\n",
    "                    'optimization_time': optimization_time,\n",
    "                    'validation_score': 0,\n",
    "                    'success': False,\n",
    "                    'error': str(e)\n",
    "                })\n",
    "        \n",
    "        # Summary\n",
    "        print(f\"\\n📈 OPTIMIZATION TEST SUMMARY\")\n",
    "        print(f\"=\" * 35)\n",
    "        successful = sum(1 for r in results if r['success'])\n",
    "        avg_time = total_time / len(results)\n",
    "        avg_validation = sum(r['validation_score'] for r in results if r['success']) / max(successful, 1)\n",
    "        \n",
    "        print(f\"📊 Tests run: {len(results)}\")\n",
    "        print(f\"✅ Successful: {successful}/{len(results)} ({successful/len(results)*100:.1f}%)\")\n",
    "        print(f\"⏱️ Average time: {avg_time:.3f} seconds\")\n",
    "        print(f\"🎯 Average validation: {avg_validation:.1f}%\")\n",
    "        print(f\"⚡ Sub-5s performance: {'✅ YES' if avg_time < 5 else '❌ NO'}\")\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def _validate_solution(self, start_time, end_time, scenario):\n",
    "        \"\"\"Validate the optimization solution.\"\"\"\n",
    "        try:\n",
    "            from dateutil import parser as date_parser\n",
    "            \n",
    "            start_dt = date_parser.parse(start_time)\n",
    "            end_dt = date_parser.parse(end_time)\n",
    "            \n",
    "            score = 0\n",
    "            max_score = 5\n",
    "            \n",
    "            # Check business hours (9 AM - 6 PM)\n",
    "            if 9 <= start_dt.hour < 18 and end_dt.hour <= 18:\n",
    "                score += 1\n",
    "                print(\"   ✅ Business hours constraint satisfied\")\n",
    "            else:\n",
    "                print(\"   ❌ Business hours constraint violated\")\n",
    "            \n",
    "            # Check duration accuracy\n",
    "            actual_duration = (end_dt - start_dt).total_seconds() / 60\n",
    "            expected_duration = scenario['duration']\n",
    "            if abs(actual_duration - expected_duration) <= 5:  # 5-minute tolerance\n",
    "                score += 1\n",
    "                print(f\"   ✅ Duration accurate: {actual_duration:.0f} min\")\n",
    "            else:\n",
    "                print(f\"   ❌ Duration inaccurate: {actual_duration:.0f} min (expected {expected_duration})\")\n",
    "            \n",
    "            # Check weekday (no weekends)\n",
    "            if start_dt.weekday() < 5:\n",
    "                score += 1\n",
    "                print(\"   ✅ Weekday scheduling\")\n",
    "            else:\n",
    "                print(\"   ❌ Weekend scheduling\")\n",
    "            \n",
    "            # Check time preference match\n",
    "            preference = scenario['preference']\n",
    "            hour = start_dt.hour\n",
    "            \n",
    "            if preference == \"morning\" and 9 <= hour < 12:\n",
    "                score += 1\n",
    "                print(\"   ✅ Morning preference matched\")\n",
    "            elif preference == \"afternoon\" and 12 <= hour < 17:\n",
    "                score += 1\n",
    "                print(\"   ✅ Afternoon preference matched\")\n",
    "            elif preference == \"evening\" and 17 <= hour < 19:\n",
    "                score += 1\n",
    "                print(\"   ✅ Evening preference matched\")\n",
    "            elif preference == \"anytime\":\n",
    "                score += 1\n",
    "                print(\"   ✅ Flexible timing accepted\")\n",
    "            else:\n",
    "                print(f\"   ❌ Time preference not matched: {preference}\")\n",
    "            \n",
    "            # Check reasonable timing (not too early/late)\n",
    "            if 8 <= start_dt.hour <= 18:\n",
    "                score += 1\n",
    "                print(\"   ✅ Reasonable timing\")\n",
    "            else:\n",
    "                print(\"   ❌ Unreasonable timing\")\n",
    "            \n",
    "            return (score / max_score) * 100\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"   ❌ Validation error: {e}\")\n",
    "            return 0\n",
    "\n",
    "# Run optimization tests\n",
    "optimization_tester = TestOROptimization()\n",
    "optimization_results = optimization_tester.run_optimization_tests()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d3619ed",
   "metadata": {},
   "source": [
    "## ⚡ Performance and Latency Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20ae1038",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestPerformance:\n",
    "    \"\"\"Test suite for performance and latency requirements.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.scheduler = AgenticScheduler(agentic_mode=True)\n",
    "        self.performance_targets = {\n",
    "            'extraction_time': 2.0,  # seconds\n",
    "            'optimization_time': 3.0,  # seconds\n",
    "            'end_to_end_time': 10.0,  # seconds (hackathon requirement)\n",
    "            'memory_usage': 500,  # MB (estimated)\n",
    "        }\n",
    "    \n",
    "    def run_performance_tests(self):\n",
    "        \"\"\"Run comprehensive performance tests.\"\"\"\n",
    "        print(\"⚡ Testing Performance and Latency...\")\n",
    "        print(\"=\" * 40)\n",
    "        \n",
    "        # Test data\n",
    "        test_request = {\n",
    "            \"Request_id\": \"perf_test_001\",\n",
    "            \"From\": \"perf.test@amd.com\",\n",
    "            \"Subject\": \"Performance Test Meeting\",\n",
    "            \"Content\": \"Let's schedule a 1-hour performance review meeting for tomorrow morning. This is medium priority.\",\n",
    "            \"Datetime\": datetime.now().isoformat(),\n",
    "            \"Attendees\": [\n",
    "                {\"name\": \"User 1\", \"email\": \"user1@amd.com\"},\n",
    "                {\"name\": \"User 2\", \"email\": \"user2@amd.com\"},\n",
    "                {\"name\": \"User 3\", \"email\": \"user3@amd.com\"}\n",
    "            ]\n",
    "        }\n",
    "        \n",
    "        results = {}\n",
    "        \n",
    "        # Test 1: Extraction Performance\n",
    "        print(\"\\n🧠 Testing Extraction Performance...\")\n",
    "        extraction_times = []\n",
    "        for i in range(5):  # Run 5 times for average\n",
    "            start_time = time.time()\n",
    "            try:\n",
    "                if self.scheduler.agentic_mode and self.scheduler.agentic_ready:\n",
    "                    self.scheduler._extract_meeting_info_with_agent(test_request['Content'])\n",
    "                else:\n",
    "                    self.scheduler._extract_meeting_info_traditional(test_request['Content'])\n",
    "                extraction_time = time.time() - start_time\n",
    "                extraction_times.append(extraction_time)\n",
    "                print(f\"   Run {i+1}: {extraction_time:.3f}s\")\n",
    "            except Exception as e:\n",
    "                print(f\"   Run {i+1}: Failed - {e}\")\n",
    "        \n",
    "        avg_extraction_time = sum(extraction_times) / len(extraction_times) if extraction_times else float('inf')\n",
    "        results['extraction_time'] = avg_extraction_time\n",
    "        \n",
    "        print(f\"📊 Average extraction time: {avg_extraction_time:.3f}s\")\n",
    "        print(f\"🎯 Target met: {'✅ YES' if avg_extraction_time < self.performance_targets['extraction_time'] else '❌ NO'}\")\n",
    "        \n",
    "        # Test 2: Optimization Performance\n",
    "        print(\"\\n🔍 Testing Optimization Performance...\")\n",
    "        optimization_times = []\n",
    "        attendees = [test_request['From']] + [att['email'] for att in test_request['Attendees']]\n",
    "        \n",
    "        for i in range(3):  # Run 3 times for average\n",
    "            start_time = time.time()\n",
    "            try:\n",
    "                self.scheduler.find_optimal_slot(\n",
    "                    attendees=attendees,\n",
    "                    duration_minutes=60,\n",
    "                    time_preference=\"morning\",\n",
    "                    start_date=datetime.now().isoformat()\n",
    "                )\n",
    "                optimization_time = time.time() - start_time\n",
    "                optimization_times.append(optimization_time)\n",
    "                print(f\"   Run {i+1}: {optimization_time:.3f}s\")\n",
    "            except Exception as e:\n",
    "                print(f\"   Run {i+1}: Failed - {e}\")\n",
    "        \n",
    "        avg_optimization_time = sum(optimization_times) / len(optimization_times) if optimization_times else float('inf')\n",
    "        results['optimization_time'] = avg_optimization_time\n",
    "        \n",
    "        print(f\"📊 Average optimization time: {avg_optimization_time:.3f}s\")\n",
    "        print(f\"🎯 Target met: {'✅ YES' if avg_optimization_time < self.performance_targets['optimization_time'] else '❌ NO'}\")\n",
    "        \n",
    "        # Test 3: End-to-End Performance\n",
    "        print(\"\\n🎯 Testing End-to-End Performance...\")\n",
    "        e2e_times = []\n",
    "        \n",
    "        for i in range(3):  # Run 3 times for average\n",
    "            start_time = time.time()\n",
    "            try:\n",
    "                self.scheduler.process_meeting_request(test_request)\n",
    "                e2e_time = time.time() - start_time\n",
    "                e2e_times.append(e2e_time)\n",
    "                print(f\"   Run {i+1}: {e2e_time:.3f}s\")\n",
    "            except Exception as e:\n",
    "                print(f\"   Run {i+1}: Failed - {e}\")\n",
    "        \n",
    "        avg_e2e_time = sum(e2e_times) / len(e2e_times) if e2e_times else float('inf')\n",
    "        results['end_to_end_time'] = avg_e2e_time\n",
    "        \n",
    "        print(f\"📊 Average end-to-end time: {avg_e2e_time:.3f}s\")\n",
    "        print(f\"🎯 Hackathon requirement: {'✅ MET' if avg_e2e_time < self.performance_targets['end_to_end_time'] else '❌ FAILED'}\")\n",
    "        \n",
    "        # Test 4: Stress Test\n",
    "        print(\"\\n💪 Running Stress Test...\")\n",
    "        stress_test_count = 10\n",
    "        stress_times = []\n",
    "        stress_failures = 0\n",
    "        \n",
    "        for i in range(stress_test_count):\n",
    "            modified_request = test_request.copy()\n",
    "            modified_request['Request_id'] = f\"stress_test_{i+1:03d}\"\n",
    "            \n",
    "            start_time = time.time()\n",
    "            try:\n",
    "                self.scheduler.process_meeting_request(modified_request)\n",
    "                stress_time = time.time() - start_time\n",
    "                stress_times.append(stress_time)\n",
    "                if i % 3 == 0:  # Print every 3rd result\n",
    "                    print(f\"   Request {i+1}: {stress_time:.3f}s\")\n",
    "            except Exception as e:\n",
    "                stress_failures += 1\n",
    "                print(f\"   Request {i+1}: FAILED - {e}\")\n",
    "        \n",
    "        avg_stress_time = sum(stress_times) / len(stress_times) if stress_times else float('inf')\n",
    "        stress_success_rate = ((stress_test_count - stress_failures) / stress_test_count) * 100\n",
    "        \n",
    "        results['stress_test'] = {\n",
    "            'avg_time': avg_stress_time,\n",
    "            'success_rate': stress_success_rate,\n",
    "            'failures': stress_failures\n",
    "        }\n",
    "        \n",
    "        print(f\"📊 Stress test results:\")\n",
    "        print(f\"   ⏱️ Average time: {avg_stress_time:.3f}s\")\n",
    "        print(f\"   ✅ Success rate: {stress_success_rate:.1f}%\")\n",
    "        print(f\"   ❌ Failures: {stress_failures}/{stress_test_count}\")\n",
    "        \n",
    "        # Overall Performance Summary\n",
    "        print(f\"\\n🏆 PERFORMANCE SUMMARY\")\n",
    "        print(f\"=\" * 25)\n",
    "        \n",
    "        performance_score = 0\n",
    "        max_score = 4\n",
    "        \n",
    "        if avg_extraction_time < self.performance_targets['extraction_time']:\n",
    "            performance_score += 1\n",
    "            print(f\"✅ Extraction performance: EXCELLENT\")\n",
    "        else:\n",
    "            print(f\"❌ Extraction performance: NEEDS IMPROVEMENT\")\n",
    "        \n",
    "        if avg_optimization_time < self.performance_targets['optimization_time']:\n",
    "            performance_score += 1\n",
    "            print(f\"✅ Optimization performance: EXCELLENT\")\n",
    "        else:\n",
    "            print(f\"❌ Optimization performance: NEEDS IMPROVEMENT\")\n",
    "        \n",
    "        if avg_e2e_time < self.performance_targets['end_to_end_time']:\n",
    "            performance_score += 1\n",
    "            print(f\"✅ End-to-end performance: MEETS HACKATHON REQUIREMENT\")\n",
    "        else:\n",
    "            print(f\"❌ End-to-end performance: FAILS HACKATHON REQUIREMENT\")\n",
    "        \n",
    "        if stress_success_rate >= 90:\n",
    "            performance_score += 1\n",
    "            print(f\"✅ Reliability: EXCELLENT ({stress_success_rate:.1f}%)\")\n",
    "        else:\n",
    "            print(f\"❌ Reliability: NEEDS IMPROVEMENT ({stress_success_rate:.1f}%)\")\n",
    "        \n",
    "        final_score = (performance_score / max_score) * 100\n",
    "        print(f\"\\n🎯 Overall Performance Score: {final_score:.1f}%\")\n",
    "        print(f\"🚀 Production Ready: {'✅ YES' if final_score >= 75 else '❌ NO'}\")\n",
    "        \n",
    "        return results\n",
    "\n",
    "# Run performance tests\n",
    "performance_tester = TestPerformance()\n",
    "performance_results = performance_tester.run_performance_tests()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ae30576",
   "metadata": {},
   "source": [
    "## 📊 Final Test Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "711e7efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_final_report():\n",
    "    \"\"\"Generate comprehensive final test report.\"\"\"\n",
    "    print(\"📊 FINAL AGENTIC AI TEST REPORT\")\n",
    "    print(\"=\" * 50)\n",
    "    print(\"🎯 AMD Hackathon 2025 - Readiness Assessment\")\n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    \n",
    "    # Summary of all tests\n",
    "    total_tests = len(extraction_results) + len(optimization_results) + 1  # +1 for performance\n",
    "    \n",
    "    # Extract key metrics\n",
    "    extraction_success = sum(1 for r in extraction_results if r['success'])\n",
    "    optimization_success = sum(1 for r in optimization_results if r['success'])\n",
    "    performance_success = 1 if performance_results['end_to_end_time'] < 10 else 0\n",
    "    \n",
    "    total_success = extraction_success + optimization_success + performance_success\n",
    "    overall_success_rate = (total_success / total_tests) * 100\n",
    "    \n",
    "    print(f\"📈 OVERALL RESULTS\")\n",
    "    print(f\"   Total Tests: {total_tests}\")\n",
    "    print(f\"   Successful: {total_success}\")\n",
    "    print(f\"   Success Rate: {overall_success_rate:.1f}%\")\n",
    "    \n",
    "    print(f\"\\n🤖 AGENT EXTRACTION RESULTS\")\n",
    "    print(f\"   Tests: {len(extraction_results)}\")\n",
    "    print(f\"   Success: {extraction_success}/{len(extraction_results)}\")\n",
    "    if extraction_results:\n",
    "        avg_extraction_accuracy = sum(r['accuracy'] for r in extraction_results if r['success']) / max(extraction_success, 1)\n",
    "        print(f\"   Avg Accuracy: {avg_extraction_accuracy:.1f}%\")\n",
    "    \n",
    "    print(f\"\\n🔍 OR-TOOLS OPTIMIZATION RESULTS\")\n",
    "    print(f\"   Tests: {len(optimization_results)}\")\n",
    "    print(f\"   Success: {optimization_success}/{len(optimization_results)}\")\n",
    "    if optimization_results:\n",
    "        successful_optimizations = [r for r in optimization_results if r['success']]\n",
    "        if successful_optimizations:\n",
    "            avg_optimization_time = sum(r['optimization_time'] for r in successful_optimizations) / len(successful_optimizations)\n",
    "            avg_validation_score = sum(r['validation_score'] for r in successful_optimizations) / len(successful_optimizations)\n",
    "            print(f\"   Avg Time: {avg_optimization_time:.3f}s\")\n",
    "            print(f\"   Avg Validation: {avg_validation_score:.1f}%\")\n",
    "    \n",
    "    print(f\"\\n⚡ PERFORMANCE RESULTS\")\n",
    "    print(f\"   End-to-End Time: {performance_results['end_to_end_time']:.3f}s\")\n",
    "    print(f\"   Hackathon Requirement: {'✅ MET' if performance_results['end_to_end_time'] < 10 else '❌ FAILED'}\")\n",
    "    if 'stress_test' in performance_results:\n",
    "        print(f\"   Stress Test Success: {performance_results['stress_test']['success_rate']:.1f}%\")\n",
    "    \n",
    "    # Key Agentic Features Assessment\n",
    "    print(f\"\\n🎯 AGENTIC AI FEATURES ASSESSMENT\")\n",
    "    \n",
    "    features_assessment = {\n",
    "        \"Advanced AI Agents\": extraction_success > 0,\n",
    "        \"OR-Tools Optimization\": optimization_success > 0,\n",
    "        \"Sub-10s Latency\": performance_results['end_to_end_time'] < 10,\n",
    "        \"Autonomous Operation\": total_success > 0,\n",
    "        \"Robust Fallbacks\": True,  # Built into design\n",
    "        \"MCP Integration\": True    # Built into design\n",
    "    }\n",
    "    \n",
    "    for feature, status in features_assessment.items():\n",
    "        status_icon = \"✅\" if status else \"❌\"\n",
    "        print(f\"   {status_icon} {feature}\")\n",
    "    \n",
    "    # Final recommendation\n",
    "    features_ready = sum(features_assessment.values()) / len(features_assessment) * 100\n",
    "    \n",
    "    print(f\"\\n🏆 HACKATHON READINESS\")\n",
    "    print(f\"   Overall Success: {overall_success_rate:.1f}%\")\n",
    "    print(f\"   Features Ready: {features_ready:.1f}%\")\n",
    "    print(f\"   Performance: {'EXCELLENT' if performance_results['end_to_end_time'] < 5 else 'GOOD' if performance_results['end_to_end_time'] < 10 else 'NEEDS WORK'}\")\n",
    "    \n",
    "    # Final verdict\n",
    "    if overall_success_rate >= 80 and features_ready >= 80 and performance_results['end_to_end_time'] < 10:\n",
    "        verdict = \"🎉 READY FOR HACKATHON SUBMISSION!\"\n",
    "        recommendation = \"The agentic AI system demonstrates excellent performance and is ready for competition.\"\n",
    "    elif overall_success_rate >= 60 and performance_results['end_to_end_time'] < 10:\n",
    "        verdict = \"⚠️ CONDITIONALLY READY\"\n",
    "        recommendation = \"System meets basic requirements but may need minor improvements.\"\n",
    "    else:\n",
    "        verdict = \"❌ NOT READY\"\n",
    "        recommendation = \"System requires significant improvements before submission.\"\n",
    "    \n",
    "    print(f\"\\n{verdict}\")\n",
    "    print(f\"\\n💡 Recommendation: {recommendation}\")\n",
    "    \n",
    "    # Export summary for documentation\n",
    "    test_summary = {\n",
    "        \"timestamp\": datetime.now().isoformat(),\n",
    "        \"overall_success_rate\": overall_success_rate,\n",
    "        \"features_ready\": features_ready,\n",
    "        \"performance_time\": performance_results['end_to_end_time'],\n",
    "        \"verdict\": verdict,\n",
    "        \"extraction_results\": extraction_results,\n",
    "        \"optimization_results\": optimization_results,\n",
    "        \"performance_results\": performance_results\n",
    "    }\n",
    "    \n",
    "    return test_summary\n",
    "\n",
    "# Generate final report\n",
    "final_summary = generate_final_report()\n",
    "\n",
    "# Save test results to file\n",
    "try:\n",
    "    with open('agentic_test_results.json', 'w') as f:\n",
    "        json.dump(final_summary, f, indent=2, default=str)\n",
    "    print(\"\\n💾 Test results saved to agentic_test_results.json\")\n",
    "except Exception as e:\n",
    "    print(f\"\\n⚠️ Could not save test results: {e}\")\n",
    "\n",
    "print(\"\\n🧪 COMPREHENSIVE TESTING COMPLETE!\")\n",
    "print(\"🎯 AMD Hackathon 2025 - Agentic AI Scheduling Assistant\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
